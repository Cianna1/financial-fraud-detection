# streaming.py
# Kafka + Spark Structured Streaming + XGBoost + Redis + durable_rules

from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, DoubleType
import redis
import json
from durable.lang import assert_fact  # æ›¿æ¢ post
import joblib
import numpy as np
import traceback
import os

# ç¯å¢ƒå˜é‡é…ç½®ï¼ˆå¦‚éœ€ï¼‰
os.environ["HADOOP_HOME"] = r"C:/Users/é™ˆä¸€å¿ƒ/hadoop"
os.environ["PATH"] += r";C:/Users/é™ˆä¸€å¿ƒ/hadoop/bin"

# 1. SparkSession
spark = SparkSession.builder \
    .appName("FraudDetectionStreaming") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
    .getOrCreate()

# 2. Kafka Topic
kafka_bootstrap = "localhost:9092"
topic = "transactions"

# 3. Schema
schema = StructType()
for i in range(1, 29):
    schema = schema.add(f"V{i}", DoubleType())
schema = schema.add("Amount", DoubleType())

# 4. æ¨¡å‹ + Redis åˆå§‹åŒ–
model, threshold = joblib.load("fraud_detector.pkl")
redis_client = redis.StrictRedis(host='localhost', port=6379, db=0)


# 6. Streaming å¤„ç†å‡½æ•°
import rules  # è‡ªåŠ¨åŠ è½½è§„åˆ™ç³»ç»Ÿ + rule_results

def process_batch(df, _):
    print("âš¡æ”¶åˆ°ä¸€æ‰¹æ•°æ®")
    df.show(truncate=False)
    records = df.collect()

    for row in records:
        data = row.asDict()
        msg_id = str(hash(str(data)))
        data["__id__"] = msg_id  # åŠ å…¥è¿½è¸ªå­—æ®µ


        # æ¨¡å‹é¢„æµ‹ï¼ˆå»é™¤ __id__ å­—æ®µï¼‰
        features = [float(data[k]) for k in data if k != "__id__"]
        vector = np.array([features])
        score = float(model.predict_proba(vector)[:, 1][0])

        # æ¸…æ´—æ•°æ®ä¾›è§„åˆ™å¼•æ“ä½¿ç”¨
        try:
            clean_data = {k: float(v) for k, v in data.items() if k != "__id__"}
            clean_data["__id__"] = msg_id
            assert_fact("fraud", clean_data)
        except Exception as e:
            print(f"âš ï¸ è§„åˆ™å¼•æ“æŠ›å‡ºå¼‚å¸¸: {repr(e)}")
            traceback.print_exc()

        # è·å–è§„åˆ™ç»“æœ
        rule_result = rules.get_rule_result(msg_id)
        hit = rule_result is not None

        if hit:
            print(f"ğŸ¯ å‘½ä¸­è§„åˆ™: {rule_result}")

        # èåˆåˆ¤æ–­
        final_flag = int(score >= threshold or hit)

        redis_client.set(f"tx:{msg_id}", json.dumps({
            "score": round(score, 4),
            "rule_hit": hit,
            "rule": rule_result['rule'] if hit else None,
            "risk": rule_result['risk'] if hit else None,
            "final_flag": final_flag
        }))
        print(f"âœ… å†™å…¥ Redis â†’ key: tx:{msg_id}, value: {{score: {score:.4f}, rule_hit: {hit}, final_flag: {final_flag}}}")


# 7. ç¼–è§£æ Kafka æµ
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_bootstrap) \
    .option("subscribe", topic) \
    .load()

json_df = kafka_df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")

query = json_df.writeStream \
    .outputMode("append") \
    .foreachBatch(process_batch) \
    .start()

print("ğŸ”¥ Spark æµå¼ä»»åŠ¡å·²å¯åŠ¨...")
query.awaitTermination()
