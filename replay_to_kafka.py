from kafka import KafkaProducer

import json
import time

# Kafka é…ç½®
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

topic = "transactions"  # ä¿æŒä¸€è‡´

# åŠ è½½å†å² Parquet æ–‡ä»¶
import pandas as pd

df = pd.read_csv("C:\\Users\\é™ˆä¸€å¿ƒ\\.kaggle\\archive\\creditcard.csv").head(20)
df.to_parquet("historical_transactions.parquet", index=False)


# æ¨¡æ‹Ÿå‘é€ï¼Œæ¯æ¡å»¶è¿Ÿ 0.5 ç§’
for idx, row in df.iterrows():
    record = row.to_dict()
    producer.send(topic, record)
    print(f"ğŸ“¤ å·²å‘é€ç¬¬ {idx+1} æ¡: {record}")
    time.sleep(0.5)

producer.flush()
print("âœ… æ‰€æœ‰å†å²è®°å½•å·²å®Œæˆå‘é€")

# ç­‰å¾… Spark Streaming å¤„ç†å®Œæœ€åä¸€æ‰¹ï¼ˆå¯é€‰å»¶è¿Ÿ 2sï¼‰
time.sleep(2)

# è¯»å– Redis ä¸­å¤„ç†ç»“æœ
import redis

r = redis.StrictRedis(host='localhost', port=6379, db=0)
print("\nğŸ§¾ Redis ä¸­å·²å¤„ç†ç»“æœé¢„è§ˆï¼ˆæœ€å¤šæ˜¾ç¤º 10 æ¡ï¼‰:")
count = 0
for key in r.scan_iter("tx:*"):
    value = r.get(key)
    print(f"ğŸ” {key.decode()}: {json.loads(value)}")
    count += 1
    if count >= 10:
        break

